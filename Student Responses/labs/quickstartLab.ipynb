{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnUQs11DflMR"
      },
      "source": [
        "# A quickstart introduction\n",
        "\n",
        "## An example ...\n",
        "As part of a [conservation effort](http://burrowingowlconservation.org/sightings/), Ann would like to report sightings of Burrowing Owls as she is hiking. Unfortunately, Ann doesn't know what a Burrowing Owl looks like so she goes to the web to look at pictures. What she has then, is a set of images that are labeled. By examining these labeled images she is **training** herself to recognize Burrowing Owls. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/owls2.png)\n",
        "\n",
        "\n",
        "More generally, we can call this set of labeled images used for training, the **labeled training dataset**. Let's dive into this idea of a labeled training dataset a bit more. Suppose Clara is given the task of distinguishing between pictures of telecaster style guitars and stratocaster. But not to worry, because her boss has given her thousands of pictures of guitars. When looking at a picture, the only thing Clara knows is that it is of either a stratocaster or a telecaster. For example, here are some pictures of stratocasters and telecasters. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/guitars2.png)\n",
        "\n",
        "\n",
        "Again, when looking at a picture all she knows is that it is a picture of a stratocaster or telecaster but she doesn't know which of the two it is. How long will it take for Clara to learn how to distinguish between these two guitar styles? Would the time be significantly shorter if her boss gave her 10,000 pictures or 100,000? If this is all the information she gets, she will never learn. What she needs is a **labeled** dataset. When presented with a picture she needs to know whether it is a picture of a stratocaster or a telecaster. \n",
        "\n",
        "\n",
        "**Now back to Ann learning to recognize Burrowing Owls**\n",
        "\n",
        "When Ann is learning to recognize Burrowing Owls from her labeled training set, she is developing a model of what features make it a Burrowing Owl. Once she is done learning she can be on a hike, see an animal, and classify it as a Burrowing Owl or something else. This is an **inference** process---based on the evidence of different features of the animal she can infer what it is. And to throw more jargon at you, this type of problem is called a **classification problem**. In classification problems the system is given the features of an object and it needs to classify that object. For example,\n",
        "\n",
        "* The features might be the words of a Twitter post (i.e., *Everything Everywhere All At Once was a f-ing masterpiece. I can't emphasize how great this movie is, it's just that great.*) and based on those features the system classifies the post as positve, negative, or neutral.\n",
        "* The features might be the pixels of an image and based on those pixels the system classifies the image into one of 1,000 categories (it's an image of an owl, a bicycle).\n",
        "\n",
        "\n",
        "\n",
        "## machine learning\n",
        "In machine learning, classification systems have a similar two step process. First is the training phase where the system uses a labeled training dataset to build a model. (We will learn about the architecture of these models and how they learn a bit later.) \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/quickDiagram2.png)\n",
        "\n",
        "\n",
        "\n",
        "> **Supervised vs. Unsupervised learning**. When a machine learning system trains on labeled data this is called supervised learning. When a system learns with unlabeled data this is called unsupervised learning. A very common example of unsupervised learning is clustering where we might give our system a million unlabeled pictures and ask it to divide the images into 10 groups. \n",
        "\n",
        "Once we have a trained model, we can use that model for inference---we can give it pictures and the system can classify them as burrowing owl or something else. Again, the two phases are:\n",
        "\n",
        "1. training\n",
        "2. inference\n",
        "\n",
        "In this introduction we are going to ignore the training phase and learn a bit about inference. To do that we are going to use a pre-trained model. What *pre-trained model* simply means is that someone else designed the architecture of the model and trained it. \n",
        "\n",
        "### AlexNet\n",
        "\n",
        "The pretrained model we will use is AlexNet. AlexNet was designed by Alex Krishevsky, Ilya Sutskever, and Geoffrey Hinton. In 2012, AlexNet won a competition where the competing systems had to classify images into one of 1,000 categories. AlexNet had a 15% error rate and the error rate of the second place winner was over 25%. Since then there are dozens of systems that perform better, but we will use AlexNet because of its historic significance. The pretrained AlexNet was trained on a labeled training dataset of over one million images.\n",
        "\n",
        "## Let's get started\n",
        "\n",
        "#### First, a note ...\n",
        "The intent of this notebook is for you to learn a little bit about data mining and have a bit of fun. The idea is not for you to understand every line of code. That will come later.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "**First let's set the runtime to GPU (Graphics Processing Unit) -- click on 'runtime' in the menu above, select 'Change runtime type' and pick 'GPU'.**\n",
        "\n",
        "Let's check to see if we set the runtime to GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xh0M2g6Tl6C",
        "outputId": "3fe8fcab-f3af-449f-bd32-5136c93c11ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 12 22:16:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXl7t6HpT2jk"
      },
      "source": [
        "You should see something like:\n",
        "\n",
        "```\n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   64C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                                  |\n",
        "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "|        ID   ID                                                   Usage      |\n",
        "|=============================================================================|\n",
        "|  No running processes found                                                 |\n",
        "+-----------------------------------------------------------------------------+\n",
        "```\n",
        "\n",
        "Showing, for example, that we are using a Tesla T4 GPU\n",
        "\n",
        "Now we will set up a variable to allow us to use the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4NoR3PrrVdBD",
        "outputId": "59aa7486-0576-48ad-e1d6-74d312de6718"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev) \n",
        "dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbSyRnWYflMY"
      },
      "source": [
        "### 1. Install Pytorch Lightning.\n",
        "First, let's install the Pytorch Lightning library on our virtual machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm_bhhMjflMY",
        "outputId": "2456a0ad-9aea-4906-e6e8-2d0bbcce17dc",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 KB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Installing collected packages: tensorboardX, torchmetrics, lightning-utilities, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.5.0 pytorch-lightning-1.8.6 tensorboardX-2.5.1 torchmetrics-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7vaBQx2flMa"
      },
      "source": [
        "The exclamation point (aka *bang*) at the beginning of the line instructs the system to interpret the rest of the line as a Unix command (something you might type in a Unix terminal).\n",
        "\n",
        "For example\n",
        "\n",
        "```!ls```\n",
        "\n",
        "will list the contents of the current directory\n",
        "\n",
        "`pip` is the **p**ackage **i**nstaller for **P**ython. As the name suggests, it install Python libraries (packages) that are not already present in the system. In this case,\n",
        "\n",
        "In the case above, we are installing the `pytorch-lightning` library.\n",
        "\n",
        "\n",
        "### 2. Import the computer vision library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jpMG96oTflMb"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpQo-Pb6flMb"
      },
      "source": [
        "Without the bang Jupyter interprets code lines in this notebook as a Python commands which\n",
        "\n",
        "```from torchvision import models``` is.\n",
        "\n",
        "`torchvision` is a library containing models and other components for computer vision. `torch` is the basic PyTorch deep learning library.\n",
        "\n",
        "There are many pretrained models available to use. Let's take a look at the possibilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uNY3j0YqflMb",
        "outputId": "fcb8e982-052a-4963-a54a-faca8c2e5883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AlexNet',\n",
              " 'AlexNet_Weights',\n",
              " 'ConvNeXt',\n",
              " 'ConvNeXt_Base_Weights',\n",
              " 'ConvNeXt_Large_Weights',\n",
              " 'ConvNeXt_Small_Weights',\n",
              " 'ConvNeXt_Tiny_Weights',\n",
              " 'DenseNet',\n",
              " 'DenseNet121_Weights',\n",
              " 'DenseNet161_Weights',\n",
              " 'DenseNet169_Weights',\n",
              " 'DenseNet201_Weights',\n",
              " 'EfficientNet',\n",
              " 'EfficientNet_B0_Weights',\n",
              " 'EfficientNet_B1_Weights',\n",
              " 'EfficientNet_B2_Weights',\n",
              " 'EfficientNet_B3_Weights',\n",
              " 'EfficientNet_B4_Weights',\n",
              " 'EfficientNet_B5_Weights',\n",
              " 'EfficientNet_B6_Weights',\n",
              " 'EfficientNet_B7_Weights',\n",
              " 'EfficientNet_V2_L_Weights',\n",
              " 'EfficientNet_V2_M_Weights',\n",
              " 'EfficientNet_V2_S_Weights',\n",
              " 'GoogLeNet',\n",
              " 'GoogLeNetOutputs',\n",
              " 'GoogLeNet_Weights',\n",
              " 'Inception3',\n",
              " 'InceptionOutputs',\n",
              " 'Inception_V3_Weights',\n",
              " 'MNASNet',\n",
              " 'MNASNet0_5_Weights',\n",
              " 'MNASNet0_75_Weights',\n",
              " 'MNASNet1_0_Weights',\n",
              " 'MNASNet1_3_Weights',\n",
              " 'MaxVit',\n",
              " 'MaxVit_T_Weights',\n",
              " 'MobileNetV2',\n",
              " 'MobileNetV3',\n",
              " 'MobileNet_V2_Weights',\n",
              " 'MobileNet_V3_Large_Weights',\n",
              " 'MobileNet_V3_Small_Weights',\n",
              " 'RegNet',\n",
              " 'RegNet_X_16GF_Weights',\n",
              " 'RegNet_X_1_6GF_Weights',\n",
              " 'RegNet_X_32GF_Weights',\n",
              " 'RegNet_X_3_2GF_Weights',\n",
              " 'RegNet_X_400MF_Weights',\n",
              " 'RegNet_X_800MF_Weights',\n",
              " 'RegNet_X_8GF_Weights',\n",
              " 'RegNet_Y_128GF_Weights',\n",
              " 'RegNet_Y_16GF_Weights',\n",
              " 'RegNet_Y_1_6GF_Weights',\n",
              " 'RegNet_Y_32GF_Weights',\n",
              " 'RegNet_Y_3_2GF_Weights',\n",
              " 'RegNet_Y_400MF_Weights',\n",
              " 'RegNet_Y_800MF_Weights',\n",
              " 'RegNet_Y_8GF_Weights',\n",
              " 'ResNeXt101_32X8D_Weights',\n",
              " 'ResNeXt101_64X4D_Weights',\n",
              " 'ResNeXt50_32X4D_Weights',\n",
              " 'ResNet',\n",
              " 'ResNet101_Weights',\n",
              " 'ResNet152_Weights',\n",
              " 'ResNet18_Weights',\n",
              " 'ResNet34_Weights',\n",
              " 'ResNet50_Weights',\n",
              " 'ShuffleNetV2',\n",
              " 'ShuffleNet_V2_X0_5_Weights',\n",
              " 'ShuffleNet_V2_X1_0_Weights',\n",
              " 'ShuffleNet_V2_X1_5_Weights',\n",
              " 'ShuffleNet_V2_X2_0_Weights',\n",
              " 'SqueezeNet',\n",
              " 'SqueezeNet1_0_Weights',\n",
              " 'SqueezeNet1_1_Weights',\n",
              " 'SwinTransformer',\n",
              " 'Swin_B_Weights',\n",
              " 'Swin_S_Weights',\n",
              " 'Swin_T_Weights',\n",
              " 'Swin_V2_B_Weights',\n",
              " 'Swin_V2_S_Weights',\n",
              " 'Swin_V2_T_Weights',\n",
              " 'VGG',\n",
              " 'VGG11_BN_Weights',\n",
              " 'VGG11_Weights',\n",
              " 'VGG13_BN_Weights',\n",
              " 'VGG13_Weights',\n",
              " 'VGG16_BN_Weights',\n",
              " 'VGG16_Weights',\n",
              " 'VGG19_BN_Weights',\n",
              " 'VGG19_Weights',\n",
              " 'ViT_B_16_Weights',\n",
              " 'ViT_B_32_Weights',\n",
              " 'ViT_H_14_Weights',\n",
              " 'ViT_L_16_Weights',\n",
              " 'ViT_L_32_Weights',\n",
              " 'VisionTransformer',\n",
              " 'Wide_ResNet101_2_Weights',\n",
              " 'Wide_ResNet50_2_Weights',\n",
              " '_GoogLeNetOutputs',\n",
              " '_InceptionOutputs',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_api',\n",
              " '_meta',\n",
              " '_utils',\n",
              " 'alexnet',\n",
              " 'convnext',\n",
              " 'convnext_base',\n",
              " 'convnext_large',\n",
              " 'convnext_small',\n",
              " 'convnext_tiny',\n",
              " 'densenet',\n",
              " 'densenet121',\n",
              " 'densenet161',\n",
              " 'densenet169',\n",
              " 'densenet201',\n",
              " 'detection',\n",
              " 'efficientnet',\n",
              " 'efficientnet_b0',\n",
              " 'efficientnet_b1',\n",
              " 'efficientnet_b2',\n",
              " 'efficientnet_b3',\n",
              " 'efficientnet_b4',\n",
              " 'efficientnet_b5',\n",
              " 'efficientnet_b6',\n",
              " 'efficientnet_b7',\n",
              " 'efficientnet_v2_l',\n",
              " 'efficientnet_v2_m',\n",
              " 'efficientnet_v2_s',\n",
              " 'get_model',\n",
              " 'get_model_builder',\n",
              " 'get_model_weights',\n",
              " 'get_weight',\n",
              " 'googlenet',\n",
              " 'inception',\n",
              " 'inception_v3',\n",
              " 'list_models',\n",
              " 'maxvit',\n",
              " 'maxvit_t',\n",
              " 'mnasnet',\n",
              " 'mnasnet0_5',\n",
              " 'mnasnet0_75',\n",
              " 'mnasnet1_0',\n",
              " 'mnasnet1_3',\n",
              " 'mobilenet',\n",
              " 'mobilenet_v2',\n",
              " 'mobilenet_v3_large',\n",
              " 'mobilenet_v3_small',\n",
              " 'mobilenetv2',\n",
              " 'mobilenetv3',\n",
              " 'optical_flow',\n",
              " 'quantization',\n",
              " 'regnet',\n",
              " 'regnet_x_16gf',\n",
              " 'regnet_x_1_6gf',\n",
              " 'regnet_x_32gf',\n",
              " 'regnet_x_3_2gf',\n",
              " 'regnet_x_400mf',\n",
              " 'regnet_x_800mf',\n",
              " 'regnet_x_8gf',\n",
              " 'regnet_y_128gf',\n",
              " 'regnet_y_16gf',\n",
              " 'regnet_y_1_6gf',\n",
              " 'regnet_y_32gf',\n",
              " 'regnet_y_3_2gf',\n",
              " 'regnet_y_400mf',\n",
              " 'regnet_y_800mf',\n",
              " 'regnet_y_8gf',\n",
              " 'resnet',\n",
              " 'resnet101',\n",
              " 'resnet152',\n",
              " 'resnet18',\n",
              " 'resnet34',\n",
              " 'resnet50',\n",
              " 'resnext101_32x8d',\n",
              " 'resnext101_64x4d',\n",
              " 'resnext50_32x4d',\n",
              " 'segmentation',\n",
              " 'shufflenet_v2_x0_5',\n",
              " 'shufflenet_v2_x1_0',\n",
              " 'shufflenet_v2_x1_5',\n",
              " 'shufflenet_v2_x2_0',\n",
              " 'shufflenetv2',\n",
              " 'squeezenet',\n",
              " 'squeezenet1_0',\n",
              " 'squeezenet1_1',\n",
              " 'swin_b',\n",
              " 'swin_s',\n",
              " 'swin_t',\n",
              " 'swin_transformer',\n",
              " 'swin_v2_b',\n",
              " 'swin_v2_s',\n",
              " 'swin_v2_t',\n",
              " 'vgg',\n",
              " 'vgg11',\n",
              " 'vgg11_bn',\n",
              " 'vgg13',\n",
              " 'vgg13_bn',\n",
              " 'vgg16',\n",
              " 'vgg16_bn',\n",
              " 'vgg19',\n",
              " 'vgg19_bn',\n",
              " 'video',\n",
              " 'vision_transformer',\n",
              " 'vit_b_16',\n",
              " 'vit_b_32',\n",
              " 'vit_h_14',\n",
              " 'vit_l_16',\n",
              " 'vit_l_32',\n",
              " 'wide_resnet101_2',\n",
              " 'wide_resnet50_2']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dir(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0IyX-BiflMc"
      },
      "source": [
        "That is a lot of pretrained models we can use!\n",
        "\n",
        "### 3. Load  the pretrained AlexNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "408e6ed3a0eb43d1948db51f813c990c",
            "d700e584272942f88cbae1821c4b87d3",
            "1f07eae3559c42398cef8007d3e582e7",
            "57b01de83f8a493286d41708f3262e52",
            "6d684fac129b4fe085b03fd8aea08f7d",
            "8274594add6e44f2a94afb1d49aa327a",
            "72298181141646aea95fec3dde7b7122",
            "35a0b44009574ff2b2899f32c7f92d5e",
            "ec5747905ba04bb182df62ba86ab4470",
            "f0a8dbc624c741f0a4c8f152ecdbee9a",
            "7f1409da879240e9a63eeb9bde1fbf5d"
          ]
        },
        "id": "lxiDfvuKflMc",
        "outputId": "47a310b6-2293-4194-99f7-85c8e2c8862c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "408e6ed3a0eb43d1948db51f813c990c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "# have model run on the GPU\n",
        "alexnet.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu7BBAs0UoEd"
      },
      "source": [
        "And let's check to see if the model is using the GPU ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5WIqgEYUse9",
        "outputId": "8f8f39a8-9dc0-4747-a6e0-d19a023c655f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "next(alexnet.parameters()).is_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yvDAoqNflMc"
      },
      "source": [
        "Excellent! CUDA is the API for NVIDIA GPUs that allow us to do parallel programming.\n",
        "\n",
        "Of course we could call this model anything we want:\n",
        "\n",
        "```\n",
        "alexnet_model = models.alexnet(pretrained=True)\n",
        "myModel = models.alexnet(pretrained=True)\n",
        "```\n",
        "\n",
        "Now we have a pretrained model loaded into our system. We would like to use the model to classify our own images. \n",
        "\n",
        "## Inference\n",
        "\n",
        "We would like to give AlexNet an image like:\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodle.jpg)\n",
        "\n",
        "and have AlexNet classify it. First, let's download the image file from the web using the Unix command `curl`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXZzt6a6flMc",
        "outputId": "a75cca88-6cfc-4877-d23e-b4eba9e62e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2796k  100 2796k    0     0  1356k      0  0:00:02  0:00:02 --:--:-- 1356k\n"
          ]
        }
      ],
      "source": [
        "!curl http://zacharski.org/files/courses/dmpics/poodle.jpg -o poodle.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umQSlZbRflMd"
      },
      "source": [
        "Next, let's load that image into Python using PIL (Python Imaging Library):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2nI-yVPYflMd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.open(\"poodle.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OTUlDW-flMd"
      },
      "source": [
        "Now let's display that image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3Mny1F_BflMe"
      },
      "outputs": [],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdD1WZQflMe"
      },
      "source": [
        "(If that doesn't display an image change the code to `img.show`)\n",
        "\n",
        "\n",
        "The size of this particular image is 4032x3024 which is slightly over 12 million pixels. That is a lot of pixels! AlexNet was designed to work with an image size of 224x224. So we need to transform the original image to AlexNet specifications by using some methods from the `torchvision` library. \n",
        "\n",
        "First we will use `transforms.Resize(256)` which transforms the image so that the smaller dimension of the original image will be resized to 256.  ([PyTorch documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html)). The resultant image will be 341x256.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodleSmall.jpg)\n",
        "\n",
        "\n",
        "To get the image to the final 224x224 size we are going to use `transforms.CenterCrop(224)` which as the name suggests crops the image at the center to a 224x224 square. The result will look something like:\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodleCropped.jpg)\n",
        "\n",
        "\n",
        "\n",
        "Then we will convert the image to an array using `ToTensor`. Since each pixel has values for red, green and blue (RGB), the resulting array will be 224x224x3. Finally we normalize the tensor using `transforms.Normalize` ([PyTorch Documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html?highlight=normalize#torchvision.transforms.Normalize)). \n",
        "\n",
        "\n",
        "\n",
        "Here are those transformations put together:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6rmI2eXBflMe"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229,0.224,0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPmTNuADflMf"
      },
      "source": [
        "You may wonder where the numbers come from in \n",
        "\n",
        "```\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229,0.224,0.225])\n",
        "```\n",
        "These are the mean and standard deviation of the RGB values for all the pixels in all the images in the ImageNet dataset.\n",
        "\n",
        "Let's use this method we defined to transform the image:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z3I_BAn9flMf"
      },
      "outputs": [],
      "source": [
        "img_t = transform(img)\n",
        "\n",
        "batch_t = torch.unsqueeze(img_t, 0)\n",
        "\n",
        "# put the tensor on the GPU\n",
        "batch_t = batch_t.to(device)\n",
        "batch_t.is_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUHn9foEflMf"
      },
      "source": [
        "Image classification models typically classify an array of images at once rather than a single image. \n",
        "\n",
        "```\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "```\n",
        "creates a tensor with one element `img_t` which itself is a tensor. In a sense, an array of image arrays.\n",
        "\n",
        "Next,\n",
        "\n",
        "```\n",
        "batch_t = batch_t.to(device)\n",
        "```\n",
        "Puts that Tensor on the GPU and\n",
        "\n",
        "```\n",
        "batch_t.is_cuda\n",
        "```\n",
        "checks to make sure that that is the case.\n",
        "\n",
        "Now we have prepared the image and are prepared to pass it to alexnet for inference.\n",
        "\n",
        "### Model Inference\n",
        "\n",
        "In PyTorch, models can be in two modes and we can toggle between them.\n",
        "\n",
        "* `alexnet.eval()` puts the model in inference mode so it can make predictions.\n",
        "* `alexnet.train()` puts the model in training mode.\n",
        "\n",
        "\n",
        "Let's get the model ready for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07sjKJ6CflMf",
        "outputId": "69f8e384-752b-4032-c992-ca129b8c136c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "alexnet.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22BjQSlflMg"
      },
      "source": [
        "As you can see, `alexnet.eval()` displays a lot of information about the architecture of the model. We will learn about the architecture of deep learning models about midway through the course.\n",
        "\n",
        "Now let's pass the tensor of our image to alexnet and get the output. Plus, let's examine the shape of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksUu8iE1flMg"
      },
      "outputs": [],
      "source": [
        "out = alexnet(batch_t)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9WSfV73flMg"
      },
      "source": [
        "As we see `out` is a one dimensional tensor with 1,000 different values. We get 1,000 values because Image_net contained 1,000 labels for the images. The larger the number the more likely the image is of that class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itIaAMZEflMh"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiH6cKiaflMh"
      },
      "source": [
        "Let's convert those values to probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UORMvTUflMh"
      },
      "outputs": [],
      "source": [
        "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP-kvbkmflMh"
      },
      "source": [
        "As you can see, the image is not very likely to be one of the first 5 labels. What are the actual names of these labels. First let's download the label name file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmDUeY4kflMi"
      },
      "outputs": [],
      "source": [
        "!curl http://zacharski.org/files/courses/dmpics/imagenet_classes.txt -o imagenet_classes.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjlBPVW9flMi"
      },
      "source": [
        "Let's load in those labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toeNd63OflMi"
      },
      "outputs": [],
      "source": [
        "with open('imagenet_classes.txt') as f:\n",
        "\n",
        "  labels = [line.strip() for line in f.readlines()]\n",
        "print(labels[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hee-TGdYflMi"
      },
      "source": [
        "The first five labels are of types of fish. It is good to know our model didn't think our image was of a fish. What does our model think?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLzeK4WIflMj"
      },
      "outputs": [],
      "source": [
        "z_, index = torch.max(out, 1)\n",
        "print(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlPw0CvPflMj"
      },
      "source": [
        "Okay, the label at index 267 is the most likely. Let's print that out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMDZWOmEflMj"
      },
      "outputs": [],
      "source": [
        "print(labels[index[0]], percentage[index[0]].item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHH57CliflMj"
      },
      "source": [
        "Fortunately, alexnet correctly thinks the image is of a standard poodle with 38.65% likelihood. \n",
        "\n",
        "The ImageNet competition evaluated systems based on the top-5 error rate meaning the system was judged correct if the correct label was among the top 5 the system predicted.  Let's look at the top 5 our model predicted for this image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL-_AsjXflMk"
      },
      "outputs": [],
      "source": [
        "_, indices = torch.sort(out, descending=True)\n",
        "[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dEP9tLjflMk"
      },
      "source": [
        "Let's turn what we learned into a function and try predicting the class of other images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3BT8abIflMk"
      },
      "outputs": [],
      "source": [
        "import  requests\n",
        "\n",
        "def predict(url):\n",
        "    # first download the image from the web\n",
        "    r = requests.get(url)\n",
        "    with open('tmp.jpg', 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    img = Image.open('tmp.jpg')\n",
        "    img.show()\n",
        "    img_t = transform(img)\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "    batch_t = batch_t.to(device)\n",
        "    out = alexnet(batch_t)\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    return([(labels[idx], percentage[idx].item()) for idx in indices[0][:5]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXBwIfxflMk"
      },
      "source": [
        "Let's find out what the 1,000 labels are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCsSr4vjflMl"
      },
      "outputs": [],
      "source": [
        "\n",
        "line = ''\n",
        "for i in range(len(labels)):\n",
        "  line += '%-25s' %labels[i]\n",
        "  if (i + 1) % 4 == 0:\n",
        "    print(line)\n",
        "    line = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYkTXbujflMl"
      },
      "source": [
        "Electric Guitar is one of the labels. Let's see if our model correctly identifies a picture of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUjOor_oflMl",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Fender_Stratocaster.jpeg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8msErztflMl"
      },
      "source": [
        "Our system is 99% sure that this image is an electric guitar.\n",
        "\n",
        "While AlexNet doesn't know about burrowing owls it does know about great horned owls. Let's give it a picture of a burrowing owl and see what it does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUMAPhl-flMm"
      },
      "outputs": [],
      "source": [
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/greyOwl.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vkazE55flMm"
      },
      "source": [
        "That is a reasonable response!\n",
        "\n",
        "Let's try a cello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bKIiLdNflMm"
      },
      "outputs": [],
      "source": [
        "predict('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cello12.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCLaql64flMm"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
        "\n",
        "\n",
        "# <font color='#EE4C2C'>You Try ...</font> \n",
        "Ok, it is time for you to try out what you just learned.\n",
        "\n",
        "## <font color='#EE4C2C'>1. Your own images</font> \n",
        "Try this out on three images of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GanSdKhSflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4342f32a-e4f2-4693-9348-c16d50f9dd20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vizsla', 16.215856552124023),\n",
              " ('Chesapeake Bay retriever', 13.272675514221191),\n",
              " ('redbone', 11.055575370788574),\n",
              " ('Irish terrier', 10.215339660644531),\n",
              " ('Rhodesian ridgeback', 4.620954990386963)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "predict('https://raw.githubusercontent.com/AgentTailhawk/PotatoVerse1_ml-class/master/Student%20Responses/labs/lab_pics/goose.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PoHaOAiZflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9dd4b3-a837-4da5-d494-0c32dece18cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('squirrel monkey', 45.073875427246094),\n",
              " ('jackfruit', 9.262455940246582),\n",
              " ('howler monkey', 6.136542320251465),\n",
              " ('spider monkey', 5.571868419647217),\n",
              " ('three-toed sloth', 4.921468257904053)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "predict('https://raw.githubusercontent.com/AgentTailhawk/PotatoVerse1_ml-class/master/Student%20Responses/labs/lab_pics/iguana.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xhck8yneflMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301c866d-9ad1-4942-beb6-43c0549d46a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('caldron', 45.687904357910156),\n",
              " ('French loaf', 7.979270935058594),\n",
              " ('eggnog', 6.931717872619629),\n",
              " ('dough', 4.421604633331299),\n",
              " ('Dutch oven', 4.0348219871521)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "predict('https://raw.githubusercontent.com/AgentTailhawk/PotatoVerse1_ml-class/master/Student%20Responses/labs/lab_pics/pie.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbFQ20DUflMn"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "## <font color='#EE4C2C'>2. Squeezenet</font> \n",
        "\n",
        "Let's try a different pretrained model, `squeezenet1_1`. Load the model, construct a function that will make predictions based on the model and try it out on the images above that we provided plus your three.</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squeezenet1_1 = models.squeezenet1_1(weights = 'SqueezeNet1_1_Weights.DEFAULT')\n",
        "\n",
        "squeezenet1_1.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weq6v4GqBfRY",
        "outputId": "57ecae3b-99c6-4873-a929-ab4e94fcfcbb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SqueezeNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (3): Fire(\n",
              "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Fire(\n",
              "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (6): Fire(\n",
              "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Fire(\n",
              "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (9): Fire(\n",
              "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): Fire(\n",
              "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): Fire(\n",
              "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): Fire(\n",
              "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (squeeze_activation): ReLU(inplace=True)\n",
              "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (expand1x1_activation): ReLU(inplace=True)\n",
              "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (expand3x3_activation): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "57or8T7-flMn"
      },
      "outputs": [],
      "source": [
        "squeezenet1_1.eval()\n",
        "\n",
        "def predict2(url):\n",
        "    # first download the image from the web\n",
        "    r = requests.get(url)\n",
        "    with open('tmp.jpg', 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    img = Image.open('tmp.jpg')\n",
        "    img.show()\n",
        "    img_t = transform(img)\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "    batch_t = batch_t.to(device)\n",
        "    out = squeezenet1_1(batch_t)\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    return([(labels[idx], percentage[idx].item()) for idx in indices[0][:5]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7eM_-FIdflMn",
        "outputId": "6006465b-9f6e-4bf0-c8b7-5c556206588c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('standard poodle', 59.634124755859375),\n",
              " ('miniature poodle', 13.934779167175293),\n",
              " ('kuvasz', 8.620738983154297),\n",
              " ('Great Pyrenees', 6.720935344696045),\n",
              " ('toy poodle', 5.138163089752197)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "predict2('http://zacharski.org/files/courses/dmpics/poodle.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5wxkMyuAflMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c158b02-dec6-4221-a55b-7d2fde5a3689"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cello', 65.8322982788086),\n",
              " ('violin', 34.16702651977539),\n",
              " ('acoustic guitar', 0.000545207760296762),\n",
              " ('hook', 6.222772935871035e-05),\n",
              " ('electric guitar', 3.828113040071912e-05)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "predict2('https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cello12.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938bd333-3e8a-48d1-c946-07ed1405c9a8",
        "id": "mbY2McRL-tbO"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vizsla', 29.568445205688477),\n",
              " ('goose', 16.279645919799805),\n",
              " ('Chesapeake Bay retriever', 10.756576538085938),\n",
              " ('redbone', 8.423840522766113),\n",
              " ('Rhodesian ridgeback', 5.551138401031494)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "predict2('https://raw.githubusercontent.com/AgentTailhawk/PotatoVerse1_ml-class/master/Student%20Responses/labs/lab_pics/goose.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "outputId": "0d10e76f-7915-478b-bb47-909b13fbbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ysxa65p-tbO"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spider monkey', 17.869428634643555),\n",
              " ('howler monkey', 15.702006340026855),\n",
              " ('three-toed sloth', 10.595609664916992),\n",
              " ('squirrel monkey', 7.942839622497559),\n",
              " ('capuchin', 6.544695854187012)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "predict2('https://raw.githubusercontent.com/AgentTailhawk/PotatoVerse1_ml-class/master/Student%20Responses/labs/lab_pics/iguana.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "outputId": "3b69ecc8-3bbc-49ec-8b33-ec3d054a2027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9h6mNPJ-tbO"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dough', 50.10575485229492),\n",
              " ('French loaf', 20.270057678222656),\n",
              " ('hotdog', 3.69765305519104),\n",
              " ('wooden spoon', 2.481151819229126),\n",
              " ('bagel', 1.535993218421936)]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "predict2('https://raw.githubusercontent.com/AgentTailhawk/PotatoVerse1_ml-class/master/Student%20Responses/labs/lab_pics/pie.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqdx9MG8flMo"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "## <font color='#EE4C2C'>2. Summary</font> \n",
        "\n",
        "Please answer the following questions by editing this markdown cell. </span>\n",
        "\n",
        "1. Classification machine learning models have two modes.<font color='#EE4C2C'> Evaluation (.eval) and Training (.train)</font> \n",
        "2. What is a pretrained model? <font color='#EE4C2C'> It's a model that has already been trained using a dataset.</font> \n",
        "2. What is supervised learning? What is unsupervised learning?<font color='#EE4C2C'> Supervised Learning is having a model train on a labeled dataset. While unsupervised learning is having a model train on an unlabeled dataset.</font> \n",
        "3. Describe in a few sentences what Squeezenet is. <font color='#EE4C2C'> Sqeezenet is a DNN (Deep Neural Network) for Computer Vision that requires fewer parameters than Alexnet and has a smaller model size.[It classifies pictures into categories.]</font> \n",
        "4. Compare the performance of AlexNet over Squeezenet. Was one more accurate than the other? Did you notice any other differences?<font color='#EE4C2C'>Squeezenet was more accurate in classifying the given images compared to AlexNet. Squeezenet has a model size that is smaller than AlexNet and requires less parameters.</font>  \n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "408e6ed3a0eb43d1948db51f813c990c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d700e584272942f88cbae1821c4b87d3",
              "IPY_MODEL_1f07eae3559c42398cef8007d3e582e7",
              "IPY_MODEL_57b01de83f8a493286d41708f3262e52"
            ],
            "layout": "IPY_MODEL_6d684fac129b4fe085b03fd8aea08f7d"
          }
        },
        "d700e584272942f88cbae1821c4b87d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8274594add6e44f2a94afb1d49aa327a",
            "placeholder": "​",
            "style": "IPY_MODEL_72298181141646aea95fec3dde7b7122",
            "value": "100%"
          }
        },
        "1f07eae3559c42398cef8007d3e582e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a0b44009574ff2b2899f32c7f92d5e",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec5747905ba04bb182df62ba86ab4470",
            "value": 244408911
          }
        },
        "57b01de83f8a493286d41708f3262e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a8dbc624c741f0a4c8f152ecdbee9a",
            "placeholder": "​",
            "style": "IPY_MODEL_7f1409da879240e9a63eeb9bde1fbf5d",
            "value": " 233M/233M [00:03&lt;00:00, 78.6MB/s]"
          }
        },
        "6d684fac129b4fe085b03fd8aea08f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8274594add6e44f2a94afb1d49aa327a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72298181141646aea95fec3dde7b7122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35a0b44009574ff2b2899f32c7f92d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5747905ba04bb182df62ba86ab4470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0a8dbc624c741f0a4c8f152ecdbee9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1409da879240e9a63eeb9bde1fbf5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}